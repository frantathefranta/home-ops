---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

x-vars: &vars
  TALOS_VERSION:
    sh: yq 'select(document_index == 1).spec.postBuild.substitute.TALOS_VERSION' {{.KUBERNETES_DIR}}/{{.cluster}}/apps/system-upgrade/system-upgrade-controller/ks.yaml
  TALOS_SCHEMATIC_ID:
    sh: yq 'select(document_index == 1).spec.postBuild.substitute.TALOS_SCHEMATIC_ID' {{.KUBERNETES_DIR}}/{{.cluster}}/apps/system-upgrade/system-upgrade-controller/ks.yaml
  KUBERNETES_VERSION:
    sh: yq 'select(document_index == 1).spec.postBuild.substitute.KUBERNETES_VERSION' {{.KUBERNETES_DIR}}/{{.cluster}}/apps/system-upgrade/system-upgrade-controller/ks.yaml
  # CONTROLLER:
  #   sh: talosctl --context {{.CLUSTER}} config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1

vars:
  TALHELPER_SECRET_FILE: "{{.CLUSTER_DIR}}/bootstrap/talos/talsecret.sops.yaml"
  TALHELPER_CONFIG_FILE: "{{.CLUSTER_DIR}}/bootstrap/talos/talconfig.yaml"
  KUBECONFIG: "{{.CLUSTER_DIR}}/kubeconfig"
  TALOSCONFIG: "{{.CLUSTER_DIR}}/bootstrap/talos/clusterconfig/talosconfig"

tasks:
  test_envs:
    vars:
      CLUSTER: '{{.CLUSTER}}'
    cmds:
      - echo "{{.KUBERNETES_DIR}}"
      - echo "{{.KUBECONFIG}}"
      - echo "{{.TALOSCONFIG}}"
      - echo "{{.CLUSTER}}"
      - echo "{{.CLUSTER_DIR}}"
      - echo "{{.TALHELPER_CONFIG_FILE}}"

    requires:
      vars: ["CLUSTER"]
    preconditions:
      - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }

  bootstrap:
    desc: Bootstrap the Talos cluster
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    prompt: Bootstrap Talos on the '{{.CLUSTER}}' cluster ... continue?
    vars: &vars
      CLUSTER: '{{.CLUSTER}}'
      TALOSCONFIG: '{{.TALOSCONFIG}}'
    cmds:
      # - task: gensecret
      #   vars: { cluster: "{{.CLUSTER}}" }
      - task: genconfig
        vars: { cluster: "{{.CLUSTER}}" }
      - task: config-apply
        vars: { cluster: "{{.CLUSTER}}" }
      - task: bootstrap-install
        vars: { cluster: "{{.CLUSTER}}" }
      - task: fetch-kubeconfig
        vars: { cluster: "{{.CLUSTER}}" }
      - task: bootstrap-apps
        vars: { cluster: "{{.CLUSTER}}" }
      - talosctl health --server=false --context {{.CLUSTER}}
    requires:
      vars: ["CLUSTER"]
    preconditions:
      - talosctl config info &>/dev/null
      - test -f {{.CLUSTER_DIR}}/bootstrap/talos/clusterconfig/talosconfig


  # gensecret:
  #   desc: Generate the Talos secrets
  #   cmds:
  #     - talhelper -f {{.CLUSTER_DIR}}/bootstrap/talos/talconfig.yaml gensecret > {{.CLUSTER_DIR}}/bootstrap/talos/talsecret.sops.yaml
  #     - task: :sops:.encrypt-file
  #       vars:
  #         file: "{{.CLUSTER_DIR}}/talos/talsecret.sops.yaml"
  #   requires:
  #     vars: ["CLUSTER"]
  #   preconditions:
  #     - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }
  #     - {
  #         msg: "Missing talhelper config file",
  #         sh: "test -f {{.TALHELPER_CONFIG_FILE}}",
  #       }
  #   status:
  #     - test -f "{{.CLUSTER_DIR}}/bootstrap/talos/talsecret.sops.yaml"

  genconfig:
    desc: Generate the Talos configs
    dir: "{{.KUBERNETES_DIR}}/{{.CLUSTER}}/bootstrap/talos/"
    cmd: talhelper genconfig -c {{.TALHELPER_CONFIG_FILE}} -s {{.TALHELPER_SECRET_FILE}} -o "{{.CLUSTER_DIR}}/bootstrap/talos/clusterconfig"
    preconditions:
      - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }
      - {
          msg: "Missing talhelper config file",
          sh: "echo {{.TALHELPER_CONFIG_FILE}} && test -f {{.TALHELPER_CONFIG_FILE}}",
        }
      - {
          msg: "Missing talhelper secret file",
          sh: "test -f {{.TALHELPER_SECRET_FILE}}",
        }

  config-apply:
    desc: Apply the Talos config on a nodes for a new cluster
    cmd: talhelper gencommand apply --extra-flags=--insecure | bash
    dir: "{{.KUBERNETES_DIR}}/{{.CLUSTER}}/bootstrap/talos/"
    preconditions:
      - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }
      - {
          msg: "Missing talhelper config file",
          sh: "test -f {{.TALHELPER_CONFIG_FILE}}",
        }
      - { msg: "Missing talconfig", sh: "test -f {{.CLUSTER_DIR}}/bootstrap/talos/clusterconfig/talosconfig" }

  bootstrap-install:
    desc: Install the Talos cluster
    cmds:
      - until talosctl --talosconfig {{.TALOSCONFIG}} --context {{.CLUSTER}} --nodes {{.CONTROLLER}} bootstrap; do sleep 10; done
      - talosctl kubeconfig --nodes {{.CONTROLLER}} --force --force-context-name {{.CLUSTER}} {{.CLUSTER_DIR}}
    vars: &vars
      CONTROLLER:
        sh: talosctl --talosconfig {{.TALOSCONFIG}} --context {{.CLUSTER}} config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    requires:
      vars: ["CLUSTER", "TALOSCONFIG"]
    preconditions:
      - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }
      - test -f {{.CLUSTER_DIR}}/bootstrap/talos/clusterconfig/talosconfig
      - talosctl --context {{.CLUSTER}} config info >/dev/null 2>&1
      # - [ "$(talosctl --context network config info -o json | jq --raw-output '.context')" = {{.CLUSTER}} ]

  bootstrap-apps:
    desc: Bootstrap core apps needed for Talos
    cmds:
      - until kubectl --kubeconfig {{.KUBERNETES_DIR}}/{{.CLUSTER}}/kubeconfig wait --for=condition=Ready=False nodes --all --timeout=600s; do sleep 10; done
      - helmfile --file {{.KUBERNETES_DIR}}/{{.CLUSTER}}/bootstrap/talos/apps/helmfile.yaml apply --skip-diff-on-install --suppress-diff --kube-context {{.cluster}}
      - until kubectl --kubeconfig {{.KUBERNETES_DIR}}/{{.CLUSTER}}/kubeconfig wait --for=condition=Ready nodes --all --timeout=600s; do sleep 10; done
    requires:
      vars: ["CLUSTER"]
    preconditions:
      - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }
      - test -f {{.KUBERNETES_DIR}}/{{.CLUSTER}}/bootstrap/talos/clusterconfig/talosconfig
      - talosctl --context {{.CLUSTER}} config info >/dev/null 2>&1
      - test -f {{.KUBERNETES_DIR}}/{{.CLUSTER}}/bootstrap/talos/apps/helmfile.yaml

  upgrade:
    desc: Upgrade Talos on a node
    cmd: bash {{.TALOS_SCRIPTS_DIR}}/upgrade.sh "{{.cluster}}" "{{.node}}" "{{.TALOS_SCHEMATIC_ID}}:{{.TALOS_VERSION}}" "{{.rollout}}"
    vars:
      rollout: '{{.rollout | default "false"}}'
    requires:
      vars: ["cluster", "node"]
    preconditions:
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap/talos/clusterconfig/{{.TALOSCONFIG}}
      - talosctl --context {{.cluster}} config info >/dev/null 2>&1
      - talosctl --context {{.cluster}} --nodes {{.node}} get machineconfig >/dev/null 2>&1

  upgrade-talos:
    desc: Upgrade talos on a node
    cmds:
      - until kubectl --context {{.cluster}} wait --timeout=5m --for=condition=Complete jobs --all --all-namespaces; do sleep 10; done
      - talosctl --context {{.cluster}} --nodes {{.node}} upgrade --image="factory.talos.dev/installer/{{.TALOS_SCHEMATIC_ID}}:{{.TALOS_VERSION}}" --wait=true --timeout=10m --preserve=true
      - talosctl --context {{.cluster}} --nodes {{.node}} health --wait-timeout=10m --server=false
    requires:
      vars: ["node", "cluster"]
    vars: *vars
    preconditions:
      - {
          msg: "Node not found",
          sh: "talosctl --nodes {{.node}} get machineconfig",
        }
      - test -f {{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap/talos/clusterconfig/{{.TALOSCONFIG}}
      - talosctl --context {{.cluster}} config info >/dev/null 2>&1

  upgrade-k8s:
    desc: Upgrade the clusters k8s version
    cmd: talosctl --context {{.cluster}} --nodes {{.controller}} upgrade-k8s --to {{.KUBERNETES_VERSION}}
    vars:
      controller:
        sh: talosctl --context {{.cluster}} config info --output json | jq --raw-output '.endpoints[0]'
    requires:
      vars: ["cluster"]
    preconditions:
      - test -f {{.TALOSCONFIG}}
      - talosctl --context {{.cluster}} config info >/dev/null 2>&1
      - talosctl --context {{.cluster}} --nodes {{.node}} get machineconfig >/dev/null 2>&1

  fetch-kubeconfig:
    desc: Fetch kubeconfig from Talos controllers
    cmd: |
      talosctl --context {{.CLUSTER}} kubeconfig --nodes {{.CONTROLLER}} \
          --force --force-context-name {{.CLUSTER}} {{.KUBERNETES_DIR}}/{{.CLUSTER}}/
    vars: *vars
    requires:
      vars: ["CLUSTER"]
    preconditions:
      - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }
      - talosctl --context {{.CLUSTER}} config info >/dev/null 2>&1
      - test -f {{.KUBERNETES_DIR}}/{{.CLUSTER}}/bootstrap/talos/clusterconfig/talosconfig

  apply-config:
    desc: Apply the Talos config on all nodes for an existing cluster
    dir: "{{.KUBERNETES_DIR}}/{{.CLUSTER}}/bootstrap/talos/"
    cmd: talhelper gencommand apply | bash
    vars:
      CLUSTER: '{{.CLUSTER}}'
    requires:
      vars: ["CLUSTER"]
    preconditions:
      - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }
      - {
          msg: "Missing talhelper config file",
          sh: "test -f {{.TALHELPER_CONFIG_FILE}}",
        }

  soft-nuke:
    desc: Resets nodes back to maintenance mode so you can re-deploy again straight after
    prompt: This will destroy your cluster and reset the nodes back to maintenance mode... continue?
    dir: "{{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap/talos/"
    cmd: talhelper gencommand reset --extra-flags "--reboot --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --graceful=false --wait=false" | bash
    preconditions:
      - { msg: "Argument (cluster) is required", sh: "test -n {{.cluster}}" }

  hard-nuke:
    desc: Resets nodes back completely and reboots them
    prompt: This will destroy your cluster and reset the nodes... continue?
    dir: "{{.KUBERNETES_DIR}}/{{.cluster}}/bootstrap/talos/"
    cmd: talhelper gencommand reset --extra-flags "--reboot --graceful=false --wait=false" | bash
    preconditions:
      - { msg: "Argument (cluster) is required", sh: "test -n {{.cluster}}" }

  node-shutdown:
    desc: Shutdown individual Talos nodes.
    requires:
      vars: [CLUSTER, HOSTNAME]
    cmds:
      - until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
      - kubectl drain {{.HOSTNAME}} --ignore-daemonsets --delete-emptydir-data --force
      - talosctl shutdown --wait=false -n {{.HOSTNAME}} --context {{.CLUSTER}}
    preconditions:
      - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }
      - { msg: "Argument (HOSTNAME) is required", sh: "test -n {{.HOSTNAME}}" }

  cluster-shutdown:
    desc: Shutdown all Talos nodes.
    prompt: This will shutdown all of the cluster nodes. Are you sure you want to continue?
    vars:
      HOSTNAMES:
        sh: |-
          kubectl get nodes --output=jsonpath='{.items[*].metadata.name}'
    cmds:
      - for: { var: HOSTNAMES }
        task: node-shutdown
        vars:
          HOSTNAME: '{{.ITEM}}'
          CLUSTER: "{{.CLUSTER}}"
    preconditions:
      - { msg: "Argument (CLUSTER) is required", sh: "test -n {{.CLUSTER}}" }

  .reset:
    internal: true
    cmd: rm -rf {{.TALOS_DIR}}
