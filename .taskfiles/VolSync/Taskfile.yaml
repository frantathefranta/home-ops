---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

# This taskfile is used to manage certain VolSync tasks for a given application, limitations are described below.
#   1. Fluxtomization, HelmRelease, PVC, ReplicationSource all have the same name (e.g. plex)
#   2. ReplicationSource and ReplicationDestination are a Restic repository
#   3. Applications are deployed as either a Kubernetes Deployment or StatefulSet
#   4. Each application only has one PVC that is being replicated

x-env-vars: &env-vars
  app: "{{.app}}"
  claim: "{{.claim}}"
  controller: "{{.controller}}"
  job: "{{.job}}"
  ns: "{{.ns}}"
  pgid: "{{.pgid}}"
  previous: "{{.previous}}"
  puid: "{{.puid}}"
  ts: "{{.ts}}"

vars:
  VOLSYNC_SCRIPTS_DIR: "{{.ROOT_DIR}}/.taskfiles/VolSync/scripts"
  VOLSYNC_TEMPLATES_DIR: "{{.ROOT_DIR}}/.taskfiles/VolSync/templates"
  KUBECONFIG: "{{.KUBERNETES_DIR}}/kubeconfig"
  VOLSYNC_RESOURCES_DIR: '{{.ROOT_DIR}}/.taskfiles/VolSync/resources'
  ts: '{{now | date "150405"}}'

tasks:
  state-*:
    desc: Suspend or Resume Volsync
    summary: |
      Args:
        cluster: Cluster to run command against (required)
        state: resume or suspend (required)
    cmds:
      - flux --context {{.cluster}} {{.state}} kustomization volsync
      - flux --context {{.cluster}} -n {{.ns}} {{.state}} helmrelease volsync
      - kubectl --context {{.cluster}} -n {{.ns}} scale deployment volsync --replicas {{if eq "suspend" .state}}0{{else}}1{{end}}
    env: *env-vars
    vars:
      ns: '{{.ns | default "storage"}}'
      state: "{{index .MATCH 0}}"
    requires:
      vars: ["cluster"]

  list:
    desc: List snapshots for an application [CLUSTER=main] [NS=default] [APP=required]
    summary: |
      Args:
        CLUSTER: Cluster to run command against (required)
        NS: Namespace the PVC is in (default: default)
        APP: Application to list snapshots for (required)
    cmds:
      - minijinja-cli --env {{.VOLSYNC_RESOURCES_DIR}}/list.yaml.j2 | kubectl apply -f -
      - kubectl -n {{.NS}} wait job/list-{{.APP}} --for condition=complete --timeout=1m
      - kubectl -n {{.NS}} logs job/list-{{.APP}} --container list
      - kubectl -n {{.NS}} delete job list-{{.APP}}
    vars:
      NS: '{{.NS | default "default"}}'
      APP: '{{.APP}}'
    env:
      NS: '{{.NS}}'
      APP: '{{.APP}}'
    requires:
      vars: [CLUSTER, APP]
    preconditions:
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/list.yaml.j2

  unlock:
    desc: Unlock all restic source repos [CLUSTER=main]
    cmds:
      - for: { var: SOURCES, split: "\n" }
        cmd: kubectl --namespace {{splitList "," .ITEM | first}} patch --field-manager=flux-client-side-apply replicationsources {{splitList "," .ITEM | last}} --type merge --patch "{\"spec\":{\"restic\":{\"unlock\":\"{{now | unixEpoch}}\"}}}"
    vars:
      SOURCES:
        sh: kubectl get replicationsources --all-namespaces --no-headers --output=jsonpath='{range .items[*]}{.metadata.namespace},{.metadata.name}{"\n"}{end}'
    requires:
      vars: [CLUSTER]
    preconditions:
      - which kubectl

  unlock-local:
    desc: Unlock a restic source repo from local machine [CLUSTER=main] [NS=default] [APP=required]
    cmds:
      - minijinja-cli {{.VOLSYNC_RESOURCES_DIR}}/unlock.yaml.j2 | kubectl apply --server-side --filename -
      - until kubectl --namespace {{.NS}} get job/volsync-unlock-{{.APP}} &>/dev/null; do sleep 5; done
      - kubectl --namespace {{.NS}} wait job/volsync-unlock-{{.APP}} --for condition=complete --timeout=5m
      - stern --namespace {{.NS}} job/volsync-unlock-{{.APP}} --no-follow
      - kubectl --namespace {{.NS}} delete job volsync-unlock-{{.APP}}
    vars:
      NS: '{{.NS | default "default"}}'
    env:
      NS: '{{.NS}}'
      APP: '{{.APP}}'
    requires:
      vars: [CLUSTER, APP]
    preconditions:
      - test -f {{.VOLSYNC_RESOURCES_DIR}}/unlock.yaml.j2
      - which kubectl minijinja-cli stern

  snapshot:
    desc: Snapshot an app [CLUSTER=main] [NS=default] [APP=required]
    cmds:
      - kubectl --namespace {{.NS}} patch replicationsources {{.APP}} --type merge -p '{"spec":{"trigger":{"manual":"{{now | unixEpoch}}"}}}'
      - until kubectl --namespace {{.NS}} get job/{{.JOB}} &>/dev/null; do sleep 5; done
      - kubectl --namespace {{.NS}} wait job/{{.JOB}} --for=condition=complete --timeout=120m
    vars:
      NS: '{{.NS | default "default"}}'
      JOB: volsync-src-{{.APP}}
    requires:
      vars: [CLUSTER, APP]
    preconditions:
      - kubectl --namespace {{.NS}} get replicationsources {{.APP}}
      - which kubectl
  # To run restore jobs in parallel for all replicationdestinations:
  #    - kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2, $1}' | xargs --max-procs=4 -l bash -c 'task volsync:restore app=$0 ns=$1'
  restore:
    desc: Restore a PVC for an application
    summary: |
      Args:
        cluster: Cluster to run command against (required)
        ns: Namespace the PVC is in (default: default)
        app: Application to restore (required)
        previous: Previous number of snapshots to restore (default: 2)
    cmds:
      - { task: .suspend, vars: *env-vars }
      - { task: .wipe, vars: *env-vars }
      - { task: .restore, vars: *env-vars }
      - { task: .resume, vars: *env-vars }
    env: *env-vars
    requires:
      vars: ["cluster", "app"]
    vars:
      ns: '{{.ns | default "default"}}'
      app: '{{ or .app (fail "Argument (app) is required") }}'
      previous: "{{.previous | default 2}}"
      controller:
        sh: "{{.VOLSYNC_SCRIPTS_DIR}}/which-controller.sh {{.app}} {{.ns}} {{.cluster}}"
      claim:
        sh: kubectl --context {{.cluster}} -n {{.ns}} get replicationsources/{{.app}} -o jsonpath="{.spec.sourcePVC}"
      puid:
        sh: kubectl --context {{.cluster}} -n {{.ns}} get replicationsources/{{.app}} -o jsonpath="{.spec.restic.moverSecurityContext.runAsUser}"
      pgid:
        sh: kubectl --context {{.cluster}} -n {{.ns}} get replicationsources/{{.app}} -o jsonpath="{.spec.restic.moverSecurityContext.runAsGroup}"
    preconditions:
      - { msg: "Argument (cluster) is required", sh: "test -n {{.cluster}}" }
      - {
          msg: "Controller script not found",
          sh: "test -f {{.VOLSYNC_SCRIPTS_DIR}}/which-controller.sh",
        }
      - {
          msg: "Wait script not found",
          sh: "test -f {{.VOLSYNC_SCRIPTS_DIR}}/wait-for-job.sh",
        }
      - {
          msg: "ReplicationDestination script not found",
          sh: "test -f {{.VOLSYNC_TEMPLATES_DIR}}/replicationdestination.tmpl.yaml",
        }
      - {
          msg: "Wipe template not found",
          sh: "test -f {{.VOLSYNC_TEMPLATES_DIR}}/wipe.tmpl.yaml",
        }

  cleanup:
    desc: Delete volume populator PVCs in all namespaces
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    cmds:
      - for: { var: dest }
        cmd: |
          {{- $items := (split "/" .ITEM) }}
          kubectl --context {{.CLUSTER}} delete pvc -n {{ $items._0 }} {{ $items._1 }}
      - for: { var: cache }
        cmd: |
          {{- $items := (split "/" .ITEM) }}
          kubectl --context {{.CLUSTER}} delete pvc -n {{ $items._0 }} {{ $items._1 }}
      - for: { var: snaps }
        cmd: |
          {{- $items := (split "/" .ITEM) }}
          kubectl --context {{.CLUSTER}} delete volumesnapshot -n {{ $items._0 }} {{ $items._1 }}
    # env: *env-vars
    requires:
      vars: ["CLUSTER"]
    vars:
      dest:
        sh: kubectl --context {{.CLUSTER}} get pvc --all-namespaces --no-headers | grep "bootstrap-dest" | awk '{print $1 "/" $2}'
      cache:
        sh: kubectl --context {{.CLUSTER}} get pvc --all-namespaces --no-headers | grep "bootstrap-cache" | awk '{print $1 "/" $2}'
      snaps:
        sh: kubectl --context {{.CLUSTER}} get volumesnapshot --all-namespaces --no-headers | grep "bootstrap-dest" | awk '{print $1 "/" $2}'

  # Suspend the Flux ks and hr
  .suspend:
    internal: true
    cmds:
      - flux --context {{.cluster}} -n {{.ns}} suspend kustomization {{.app}}
      - flux --context {{.cluster}} -n {{.ns}} suspend helmrelease {{.app}}
      - kubectl --context {{.cluster}} -n {{.ns}} scale {{.controller}} --replicas 0
      - kubectl --context {{.cluster}} -n {{.ns}} wait pod --for delete --selector="app.kubernetes.io/name={{.app}}" --timeout=2m
    env: *env-vars

  # Wipe the PVC of all data
  .wipe:
    internal: true
    cmds:
      - envsubst < <(cat {{.VOLSYNC_TEMPLATES_DIR}}/wipe.tmpl.yaml) | kubectl --context {{.cluster}} apply -f -
      - bash {{.VOLSYNC_SCRIPTS_DIR}}/wait-for-job.sh wipe-{{.app}}-{{.ts}} {{.ns}} {{.cluster}}
      - kubectl --context {{.cluster}} -n {{.ns}} wait job/wipe-{{.app}}-{{.ts}} --for condition=complete --timeout=120m
      - kubectl --context {{.cluster}} -n {{.ns}} logs job/wipe-{{.app}}-{{.ts}} --container main
      - kubectl --context {{.cluster}} -n {{.ns}} delete job wipe-{{.app}}-{{.ts}}
    env: *env-vars
    vars:
      job: volsync-wipe-{{.app}}

  # Create VolSync replicationdestination CR to restore data
  .restore:
    internal: true
    cmds:
      - envsubst < <(cat {{.VOLSYNC_TEMPLATES_DIR}}/replicationdestination.tmpl.yaml) | kubectl --context {{.cluster}} apply -f -
      - bash {{.VOLSYNC_SCRIPTS_DIR}}/wait-for-job.sh volsync-dst-{{.app}}-{{.ts}} {{.ns}} {{.cluster}}
      - kubectl --context {{.cluster}} -n {{.ns}} wait job/volsync-dst-{{.app}}-{{.ts}} --for condition=complete --timeout=120m
      - kubectl --context {{.cluster}} -n {{.ns}} delete replicationdestination {{.app}}-{{.ts}}
    env: *env-vars
    vars:
      job: "{{.app}}"

  # Resume Flux ks and hr
  .resume:
    internal: true
    cmds:
      - kubectl --context {{.cluster}} -n {{.ns}} scale {{.controller}} --replicas 1
      - flux --context {{.cluster}} -n {{.ns}} resume helmrelease {{.app}}
      - flux --context {{.cluster}} -n {{.ns}} resume kustomization {{.app}}
    env: *env-vars
