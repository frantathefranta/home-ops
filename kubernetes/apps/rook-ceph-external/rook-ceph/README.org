#+title: Rook Ceph
I took it as a challenge to materialize [[https://onedr0p.github.io/home-ops/notes/proxmox-considerations.html][onedr0p's idea]] to have an external Ceph cluster for my Kubernetes cluster to consume. It's pretty simple surprisingly, but I did run into some hurdles.
* External cluster
** Specifications
| *Server*  | Dell R630                                                            |
| *Network* | ConnectX-4 100GbE cards in a mesh configuration, 10Gb public network |
| *OSDs*    | 3x 800GB Enterprise SSD in each server                               |
** Prerequisites
I set up my external cluster on [[https://pve.proxmox.com/wiki/Deploy_Hyper-Converged_Ceph_Cluster][Proxmox]]. One gotcha that I've ran into is that when you use the [[https://pve.proxmox.com/wiki/Full_Mesh_Network_for_Ceph_Server][Full Mesh Network]], you'll need to be careful that the ~public_network~ in ~ceph.conf~ is set to a reachable network for your Kubernetes cluster. If it's not, you'll ran into weird issues that are more difficult to diagnose (at least for me). It's good to check if your CSI provisioners are trying to reach the right endpoints (Cillium Hubble is pretty great for this if you use this CNI).
* Kubernetes steps
** Running the ~create-external-cluster-resources.py~ script
My command looks like this:
#+begin_src sh :noeval
$ python3 create-external-cluster-resources.py --rbd-data-pool-name talos-maxi-pv  --namespace rook-ceph-external --format bash --monitoring-endpoint 10.40.1.50  --cephfs-filesystem-name talos-maxi-fs --v2-port-enable --rgw-endpoint 10.40.1.50:7480
#+end_src
*** Notes
 * ~--rgw-endpoint~ is only necessary if you want to add S3 ObjectStore functionality. [[https://pve.proxmox.com/wiki/User:Grin/Ceph_Object_Gateway][Guide on how to set that up on Proxmox]]
** Running the ~import-external-cluster.sh~ script
When your ~operator~ and ~cluster~ HelmReleases are deployed, you can run this script. Make sure that you have the right ~OPERATOR_NAMESPACE~ env var set in the script, or export a new env ~CSI_DRIVER_NAME_PREFIX~ if you have a non-standard namespace (like I do). If this is set wrong, your provisioner will be named wrong and your PVCs will not start provisioning.
** Importing the cluster information automatically
Alternatively, if you don't want to run ~import-external-cluster.sh~ on each cluster bootstrap, you can create these files:
 - ~rbd-secrets.yaml~
 - ~cephfs-secrets.yaml~
 - ~storageclass.yaml~
 - ~monitor-config.yaml~
   and if you want to add the Object Store, add ~rgw-secret.yaml~

You can select how these secrets get populated with values, I chose to add them in ~<cluster>/flux/vars/cluster-secrets.sops.yaml~ and ~shared/settings/cluster-secrets.sops.yaml~ (flux will populate the ~${VALUE}~ fields then).
* Ceph Object Store
It's possible to also connect to the Rados GW from the *Kubernetes* cluster. I've chosen to use *HAProxy* to load balance the 3 endpoints of my nodes. An important thing there is to set the ~rgw_dns_name~ to the hostname that you set for the ~rook-ceph-cluster~ deployment. In my case, I just set it to ~haproxy-rados-gw.rook-ceph-external.svc.cluster.local~. If you don't do this, you'll get this kind of error:
#+begin_src shell
$ curl haproxy-rados-gw.rook-ceph-external.svc.cluster.local:7480
<?xml version="1.0" encoding="UTF-8"?><Error><Code>NoSuchBucket</Code><Message></Message><BucketName>haproxy-rados-gw.rook-ceph-external.svc.cluster.local</BucketName><RequestId>tx000000b62ae3c49af5eab-0067474692-28035997-default</RequestId><HostId>280359
#+end_src

What you want to see is this:
#+begin_src shell
$ curl haproxy-rados-gw.rook-ceph-external.svc.cluster.local:7480
<?xml version="1.0" encoding="UTF-8"?><ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Owner><ID>anonymous</ID></Owner><Buckets></Buckets></ListAllMyBucketsResult>
#+end_src
* Caveats
+Your backups will not be handled by the Kubernetes cluster.+
I figured out you can use ~VolumeSnapshots~ with an external cluster, description WIP.
